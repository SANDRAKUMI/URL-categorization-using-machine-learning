{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/domantas/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package words to /home/domantas/nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/domantas/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/domantas/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "import ast\n",
    "import numpy as np\n",
    "import os\n",
    "import ast\n",
    "import urllib.request\n",
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup\n",
    "import os.path\n",
    "from datetime import datetime\n",
    "from collections import Counter\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('words')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "month = \"january\"\n",
    "df = pd.read_csv(\"../Datasets/full_data_january.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "People_and_Society\n",
      "Reference\n",
      "Home_and_Garden\n",
      "Science\n",
      "Adult\n",
      "Internet_and_Telecom\n",
      "Travel\n",
      "Shopping\n",
      "Beauty_and_Fitness\n",
      "Autos_and_Vehicles\n",
      "Games\n",
      "Career_and_Education\n",
      "Recreation_and_Hobbies\n",
      "Law_and_Government\n",
      "Gambling\n",
      "Finance\n",
      "Sports\n",
      "Pets_and_Animals\n",
      "Health\n",
      "Business_and_Industry\n",
      "Books_and_Literature\n",
      "Food_and_Drink\n",
      "News_and_Media\n",
      "Arts_and_Entertainment\n",
      "Computer_and_Electronics\n"
     ]
    }
   ],
   "source": [
    "top = 2500\n",
    "words_frequency = {}\n",
    "for category in set(df['main_category'].values):\n",
    "    print(category)\n",
    "    all_words = []\n",
    "    for row in df[df['main_category'] == category]['tokenized_words'].tolist():\n",
    "        for word in ast.literal_eval(row):\n",
    "            all_words.append(word)\n",
    "    most_common = nltk.FreqDist(w for w in all_words).most_common(top)\n",
    "    words_frequency[category] = most_common"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "for category in set(df['main_category'].values):\n",
    "    words_frequency[category] = [word for word, number in words_frequency[category]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "features = np.zeros(df.shape[0] * top).reshape(df.shape[0], top)\n",
    "labels = np.zeros(df.shape[0])\n",
    "counter = 0\n",
    "for i, row in df.iterrows():\n",
    "    c = [word for word, word_count in Counter(ast.literal_eval(row['tokenized_words'])).most_common(top)]\n",
    "    labels[counter] = list(set(df['main_category'].values)).index(row['main_category'])\n",
    "    for word in c:\n",
    "        if word in words_frequency[row['main_category']]:\n",
    "            features[counter][words_frequency[row['main_category']].index(word)] = 1\n",
    "    counter += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/domantas/.local/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/home/domantas/.local/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:460: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression\n",
      "Score:  0.8547826086956521\n",
      "Top:  2500\n",
      "Dataset length:  10452\n",
      "\n",
      "SVM\n",
      "Score:  0.7884057971014493\n",
      "Top:  2500\n",
      "Dataset length:  10452\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/domantas/.local/lib/python3.6/site-packages/sklearn/svm/base.py:922: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from scipy.sparse import coo_matrix\n",
    "X_sparse = coo_matrix(features)\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "X, X_sparse, y = shuffle(features, X_sparse, labels, random_state=0)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "lr = LogisticRegression()\n",
    "lr.fit(X_train, y_train)\n",
    "predictions = lr.predict(X_test)\n",
    "score = lr.score(X_test, y_test)\n",
    "print('LogisticRegression')\n",
    "print('Score: ', score)\n",
    "print('Top: ', top)\n",
    "print('Dataset length: ', df.shape[0])\n",
    "print()\n",
    "\n",
    "from sklearn.svm import LinearSVC\n",
    "clf = LinearSVC()\n",
    "clf.fit(X_train, y_train)\n",
    "predictions = clf.predict(X_test)\n",
    "score = clf.score(X_test, y_test)\n",
    "print('SVM')\n",
    "print('Score: ', score)\n",
    "print('Top: ', top)\n",
    "print('Dataset length: ', df.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearSVC(C=1.0, class_weight=None, dual=True, fit_intercept=True,\n",
       "     intercept_scaling=1, loss='squared_hinge', max_iter=1000,\n",
       "     multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,\n",
       "     verbose=0)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "          n_jobs=None, penalty='l2', random_state=None, solver='warn',\n",
       "          tol=0.0001, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save models\n",
    "from sklearn.externals import joblib\n",
    "filename = \"../Models/{}/LR_model_{}.joblib\".format(month.title(), month)\n",
    "if not os.path.isfile(filename):\n",
    "    joblib.dump(lr, filename)\n",
    "    \n",
    "filename = \"../Models/{}/LSVM_model_{}.joblib\".format(month.title(), month)\n",
    "if not os.path.isfile(filename):\n",
    "    joblib.dump(clf, filename)\n",
    "\n",
    "import pickle\n",
    "words_filename = \"../Models/{}/word_frequency_{}.picle\".format(month.title(), month)\n",
    "if not os.path.isfile(words_filename):\n",
    "    pickle_out = open(words_filename,\"wb\")\n",
    "    pickle.dump(words_frequency, pickle_out)\n",
    "    pickle_out.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "pickle_in = open(\"../Models/{}/word_frequency_{}.picle\".format(month.title(), month),\"rb\")\n",
    "words_frequency_2 = pickle.load(pickle_in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "temp = copy.deepcopy(words_frequency)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_frequency = copy.deepcopy(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove chunk words\n",
    "# from math import floor\n",
    "# words = []\n",
    "# for category in words_frequency.keys():\n",
    "#     words.extend(words_frequency[category][0:20])\n",
    "# words_counter = Counter(words)\n",
    "# chunk_words = [x for x in words_counter if words_counter[x] >= 12]\n",
    "# words_filter = {x : words_counter[x] for x in words_counter if words_counter[x] >= 12}\n",
    "# for cat in words_frequency.keys():\n",
    "#     words_frequency[cat] = [word for word in words_frequency[cat] if word not in chunk_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_custom = pd.read_csv(\"../Datasets/custom_dataset.csv\")[['URL', 'Category', 'Language']]\n",
    "df_custom = df_custom[df_custom['URL'].notnull()]\n",
    "df_custom['Weight_model'] = ''\n",
    "df_custom['lr_normal'] = ''\n",
    "df_custom['lr_max'] = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "top = 2500\n",
    "toker = RegexpTokenizer(r'((?<=[^\\w\\s])\\w(?=[^\\w\\s])|(\\W))+', gaps=True)\n",
    "char_blacklist = list(chr(i) for i in range(32, 127) if i <= 64 or i >= 91 and i <= 96 or i >= 123)\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "stopwords.extend(char_blacklist)\n",
    "hdr = {'User-Agent': 'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.11 (KHTML, like Gecko) Chrome/23.0.1271.64 Safari/537.11',\n",
    "       'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',\n",
    "       'Accept-Charset': 'ISO-8859-1,utf-8;q=0.7,*;q=0.3',\n",
    "       'Accept-Encoding': 'none',\n",
    "       'Accept-Language': 'en-US,en;q=0.8',\n",
    "       'Connection': 'keep-alive'}\n",
    "row_counter = 0 \n",
    "for row_id, row in df_custom.iterrows():\n",
    "    row_counter += 1\n",
    "    try:\n",
    "        url = row['URL']\n",
    "        req = urllib.request.Request(url, headers=hdr)\n",
    "        html = urlopen(req, timeout=15).read()\n",
    "        # html = urlopen(url, timeout=15).read()\n",
    "        soup = BeautifulSoup(html, \"html.parser\")\n",
    "        [tag.decompose() for tag in soup(\"script\")]\n",
    "        [tag.decompose() for tag in soup(\"style\")]\n",
    "        text = soup.get_text()\n",
    "        lines = (line.strip() for line in text.splitlines())\n",
    "        chunks = (phrase.strip() for line in lines for phrase in line.split(\"  \"))\n",
    "        text = '\\n'.join(chunk.lower() for chunk in chunks if chunk)\n",
    "        # Tokenize text\n",
    "        tokens = [token.lower() for token in toker.tokenize(text)]\n",
    "        # Remove stopwords\n",
    "        wnl = WordNetLemmatizer()\n",
    "        tokens = [token.lower() for token in toker.tokenize(text)]\n",
    "        tokens_stopwords = [w.lower() for w in tokens if w not in stopwords and len(w) >= 3 and w[0] not in char_blacklist]\n",
    "        tokens_lemmatize = [wnl.lemmatize(token) for token in tokens_stopwords]\n",
    "        \n",
    "        from collections import Counter\n",
    "        counter = 0\n",
    "        features_pred = np.zeros(top * len(words_frequency)).reshape(len(words_frequency), top)\n",
    "        c = [word for word, word_count in Counter(tokens_lemmatize).most_common(top)]\n",
    "        for category in words_frequency.keys():\n",
    "            for word in c:\n",
    "                if word in words_frequency[category]:\n",
    "                    features_pred[counter][words_frequency[category].index(word)] = 1\n",
    "            counter+=1\n",
    "\n",
    "        category_weight = []\n",
    "        for i in features_pred:\n",
    "            weight_cof = np.where(i == 1)[0]\n",
    "            weight_sum = 0\n",
    "            for cof in weight_cof:\n",
    "                weight_sum += top - cof\n",
    "            category_weight.append(weight_sum)\n",
    "\n",
    "        cat_index = category_weight.index(max(category_weight))\n",
    "        category = list(words_frequency.keys())[cat_index]\n",
    "        feature = features_pred[cat_index].reshape(-1, top)\n",
    "        print(\"url: {} . {} / {}\".format(url, row_counter, len(df_custom)))\n",
    "        print('Category: ', row['Category'])\n",
    "        print(\"My model: \",category)\n",
    "        prediction = lr.predict(feature)\n",
    "        print(\"LR normal: \", list(words_frequency.keys())[int(prediction[0])])\n",
    "        df_custom.at[row_id, 'Weight_model'] = category\n",
    "        df_custom.at[row_id, 'lr_normal'] = list(words_frequency.keys())[int(prediction[0])]\n",
    "    except:\n",
    "        print(\"{} - Failed. {} / {}\".format(row['URL'], row_counter, len(df_custom)))\n",
    "        continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_custom = df_custom[df_custom['Weight_model'] != '']\n",
    "model_acc = len(df_custom[df_custom['Weight_model'] == df_custom['Category']]) / len(df_custom) * 100\n",
    "lr_acc = len(df_custom[df_custom['lr_normal'] == df_custom['Category']]) / len(df_custom) * 100\n",
    "print(\"My model accuracy: {}% | {} / {}\".format(model_acc, len(df_custom[df_custom['Weight_model'] == df_custom['Category']]), len(df_custom)))\n",
    "print(\"Logistic regression accuracy: {}% | {} / {}\".format(lr_acc, len(df_custom[df_custom['lr_normal'] == df_custom['Category']]), len(df_custom)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for category in list(set(df_custom['Category'].values)):\n",
    "    print(category)\n",
    "    df2 = df_custom[df_custom['Category'] == category]\n",
    "    result_w = len(df2[df2['Weight_model'] == category])\n",
    "    result_l = len(df2[df2['lr_normal'] == category])   \n",
    "    print(\"Model: {} / {} : {:.2f}%\".format(result_w, len(df2), result_w / len(df2) * 100))\n",
    "    print(\"lr: {} / {} : {:.2f}%\".format(result_l, len(df2), result_l / len(df2) * 100))\n",
    "    print(\"*\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
