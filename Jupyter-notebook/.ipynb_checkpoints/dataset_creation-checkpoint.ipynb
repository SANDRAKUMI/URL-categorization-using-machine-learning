{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/domantas/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package words to /home/domantas/nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/domantas/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "import ast\n",
    "import numpy as np\n",
    "import os\n",
    "import ast\n",
    "import urllib.request\n",
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup\n",
    "import os.path\n",
    "from datetime import datetime\n",
    "from collections import Counter\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('words')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"/home/domantas/Desktop/University/bachelor_url_categorization\")\n",
    "month = \"december_test\"\n",
    "char_blacklist = list(chr(i) for i in range(32, 127) if i <= 64 or i >= 91 and i <= 96 or i >= 123)\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "stopwords.extend(char_blacklist)\n",
    "english_vocab = set(w.lower() for w in nltk.corpus.words.words())\n",
    "english_tolerance = 50\n",
    "english_confidence = []\n",
    "words_threshold = 15\n",
    "top = 2500\n",
    "toker = RegexpTokenizer(r'((?<=[^\\w\\s])\\w(?=[^\\w\\s])|(\\W))+', gaps=True)\n",
    "words_frequency = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def no_filter_data():\n",
    "    file = 'Datasets/URL-categorization-DFE.csv'\n",
    "    df = pd.read_csv(file)[['main_category', 'main_category:confidence', 'url']]\n",
    "    df = df[(df['main_category'] != 'Not_working') & (df['main_category:confidence'] > 0.5)][:200]\n",
    "    df['tokenized_words'] = ''\n",
    "    counter = 0\n",
    "    for i, row in df.iterrows():\n",
    "        counter += 1\n",
    "        print(\"{}, {}/{}; Time: {}\".format(row['url'], counter, len(df), str(datetime.now())))\n",
    "        try:\n",
    "            hdr = {'User-Agent': 'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.11 (KHTML, like Gecko) Chrome/23.0.1271.64 Safari/537.11',\n",
    "               'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',\n",
    "               'Accept-Charset': 'ISO-8859-1,utf-8;q=0.7,*;q=0.3',\n",
    "               'Accept-Encoding': 'none',\n",
    "               'Accept-Language': 'en-US,en;q=0.8',\n",
    "               'Connection': 'keep-alive'}\n",
    "            req = urllib.request.Request('http://' + row['url'], headers=hdr)\n",
    "            html = urlopen(req, timeout=15).read()\n",
    "        except:\n",
    "            print(\"{} Failed\".format(row['url']))\n",
    "            continue\n",
    "\n",
    "        soup = BeautifulSoup(html, \"html.parser\")\n",
    "        [tag.decompose() for tag in soup(\"script\")]\n",
    "        [tag.decompose() for tag in soup(\"style\")]\n",
    "        text = soup.get_text()\n",
    "        lines = (line.strip() for line in text.splitlines())\n",
    "        chunks = (phrase.strip() for line in lines for phrase in line.split(\"  \"))\n",
    "        text = '\\n'.join(chunk.lower() for chunk in chunks if chunk)\n",
    "        # Tokenize text\n",
    "        wnl = WordNetLemmatizer()\n",
    "        tokens = [token.lower() for token in toker.tokenize(text)]\n",
    "        # Remove stopwords\n",
    "        tokens = [token.lower() for token in toker.tokenize(text)]\n",
    "        tokens_stopwords = [w.lower() for w in tokens if w not in stopwords and len(w) >= 3 and w[0] not in char_blacklist]\n",
    "        tokens_lemmatize = [wnl.lemmatize(token) for token in tokens_stopwords]\n",
    "        # Calculate percentage of english words\n",
    "        english_tokens = []\n",
    "        for word in tokens_lemmatize:\n",
    "            if word.lower() in english_vocab:\n",
    "                english_tokens.append(word.lower())\n",
    "        english_confidence = len(english_tokens) / len(tokens_lemmatize) * 100 if len(english_tokens) > 0 else 0\n",
    "        if len(english_tokens) < words_threshold or english_confidence < english_tolerance:\n",
    "            continue\n",
    "        # Track frequent words list for each category\n",
    "        cat = row['main_category']\n",
    "        if cat not in [c for c in words_frequency.keys()]:\n",
    "            words_frequency[cat] = []\n",
    "        words_frequency[row['main_category']] = words_frequency[row['main_category']] + english_tokens\n",
    "        \n",
    "        df.at[i, 'tokenized_words'] = english_tokens\n",
    "        df.at[i, 'english:confidence'] = english_confidence\n",
    "        \n",
    "    df = df[~df['tokenized_words'].isnull()]\n",
    "    df.to_csv(\"Datasets/full_data_{}.csv\".format(month))\n",
    "if not os.path.isfile(\"Datasets/full_data_{}.csv\".format(month)):\n",
    "    no_filter_data()\n",
    "df = pd.read_csv(\"Datasets/full_data_{}.csv\".format(month))\n",
    "df = df[df['english:confidence'] > english_tolerance]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Count words frequency for each category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_frequency = {}\n",
    "for category in set(df['main_category'].values):\n",
    "    all_words = []\n",
    "    for row in df[df['main_category'] == category]['tokenized_words'].tolist():\n",
    "        all_words = all_words + ast.literal_eval(row)\n",
    "    allWordExceptStopDist = nltk.FreqDist(w.lower() for w in all_words)\n",
    "    most_common = allWordExceptStopDist.most_common(top)\n",
    "    words_frequency[category] = [word for word, number in most_common]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create labels and features set for ML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = np.zeros(df.shape[0] * top).reshape(df.shape[0], top)\n",
    "labels = np.zeros(df.shape[0])\n",
    "counter = 0\n",
    "for i, row in df.iterrows():\n",
    "    c = [word for word, word_count in Counter(ast.literal_eval(row['tokenized_words'])).most_common(top)]\n",
    "    labels[counter] = list(set(df['main_category'].values)).index(row['main_category'])\n",
    "    for word in c:\n",
    "        if word in words_frequency[row['main_category']]:\n",
    "            features[counter][words_frequency[row['main_category']].index(word)] = 1\n",
    "    counter += 1\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from scipy.sparse import coo_matrix\n",
    "X_sparse = coo_matrix(features)\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "X, X_sparse, y = shuffle(features, X_sparse, labels, random_state=0)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train and validate data using ML algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "lr = LogisticRegression()\n",
    "lr.fit(X_train, y_train)\n",
    "predictions = lr.predict(X_test)\n",
    "score = lr.score(X_test, y_test)\n",
    "print('LogisticRegression')\n",
    "print('Score: ', score)\n",
    "print('Top: ', top)\n",
    "print('Tolerance: ', english_tolerance)\n",
    "print('Dataset length: ', df.shape[0])\n",
    "print()\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "dtc = DecisionTreeClassifier()\n",
    "dtc.fit(X_train, y_train)\n",
    "predictions = dtc.predict(X_test)\n",
    "score = dtc.score(X_test, y_test)\n",
    "print('DecisionTreeClassifier')\n",
    "print('Score: ', score)\n",
    "print('Top: ', top)\n",
    "print('Tolerance: ', english_tolerance)\n",
    "print('Dataset length: ', df.shape[0])\n",
    "print()\n",
    "from sklearn.svm import LinearSVC\n",
    "clf = LinearSVC()\n",
    "clf.fit(X_train, y_train)\n",
    "predictions = clf.predict(X_test)\n",
    "score = clf.score(X_test, y_test)\n",
    "print('SVM')\n",
    "print('Score: ', score)\n",
    "print('Top: ', top)\n",
    "print('Tolerance: ', english_tolerance)\n",
    "print('Dataset length: ', df.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.externals import joblib\n",
    "filename = \"Models/LR_model_{}.joblib\".format(month)\n",
    "if not os.path.isfile(filename):\n",
    "    joblib.dump(lr, filename)\n",
    "\n",
    "import pickle\n",
    "words_filename = \"Models/word_frequency_{}.picle\".format(month)\n",
    "if not os.path.isfile(words_filename):\n",
    "    pickle_out = open(words_filename,\"wb\")\n",
    "    pickle.dump(words_frequency, pickle_out)\n",
    "    pickle_out.close()\n",
    "\n",
    "filename = \"Models/LR_maxtrain_{}.joblib\".format(month)\n",
    "if not os.path.isfile(filename):\n",
    "    from sklearn.linear_model import LogisticRegression\n",
    "    lr = LogisticRegression()\n",
    "    lr.fit(X, y)\n",
    "    joblib.dump(lr, filename)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
