{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import libraries\n",
    "These libraries will be used for our URL_classification project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14:23:30.843081\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "import csv\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import ast\n",
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup\n",
    "import os.path\n",
    "\n",
    "print(datetime.datetime.now().time())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Use this command if you have any errors on importing nltk library. It will open a nltk meniu with download and update options. If it's still missing some libraries, it needs to install manually by writing nltk.download('library name') where library name is missing library name which asserts error message."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/domantas/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package words to /home/domantas/nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/domantas/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "nltk.download('words')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conctruct_dataset():\n",
    "    file = 'URL-categorization-DFE.csv'\n",
    "    df = pd.read_csv(file)[['main_category', 'main_category:confidence', 'url']]\n",
    "    df = df[(df['main_category'] != 'Not_working') & (df['main_category:confidence'] > 0.5)]\n",
    "\n",
    "    char_blacklist = list(chr(i) for i in range(32, 127) if i <= 64 or i >= 91 and i <= 96 or i >= 123)\n",
    "    stopwords = nltk.corpus.stopwords.words('english')\n",
    "    stopwords.extend(char_blacklist)\n",
    "    language_whitelist = ['en']\n",
    "    english_vocab = set(w.lower() for w in nltk.corpus.words.words())\n",
    "    blacklist_domain = ['.it', '.ru', '.cn', '.jp', '.tw', '.de', '.pl', '.fr', '.hu', '.bg', '.nl']\n",
    "\n",
    "    df = df[~df['url'].str.endswith(tuple(blacklist_domain))]\n",
    "    df['tokenized_words'] = ''\n",
    "    \n",
    "    counter = 0\n",
    "    for i, row in df.iterrows():\n",
    "        counter += 1\n",
    "        if counter >= 50:\n",
    "            break\n",
    "        print(\"{}, {}/{}\".format(row['url'], counter, len(df)))\n",
    "\n",
    "        try:\n",
    "            html = urlopen('http://' + row['url'], timeout=1).read()\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "        soup = BeautifulSoup(html, \"html.parser\")\n",
    "        for script in soup([\"script\", \"style\"]):\n",
    "            script.extract()\n",
    "        text = soup.get_text()\n",
    "        lines = (line.strip() for line in text.splitlines())\n",
    "        chunks = (phrase.strip() for line in lines for phrase in line.split(\"  \"))\n",
    "        text = '\\n'.join(chunk.lower() for chunk in chunks if chunk)\n",
    "        filter_text = \" \".join(w for w in nltk.word_tokenize(text) \\\n",
    "             if w.lower() in english_vocab)\n",
    "\n",
    "        tokens = nltk.word_tokenize(filter_text)\n",
    "\n",
    "        allWordExceptStopDist = nltk.FreqDist(\n",
    "            w.lower() for w in tokens if w not in stopwords and len(w) >= 3 and w[0] not in char_blacklist)\n",
    "\n",
    "        all_words = [i for i in allWordExceptStopDist]\n",
    "\n",
    "        if len(all_words) > 0:\n",
    "            continue\n",
    "\n",
    "        df.at[i, 'tokenized_words'] = all_words\n",
    "        \n",
    "    df = df[df['tokenized_words'] != '']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_machine():\n",
    "    char_blacklist = list(chr(i) for i in range(32, 127) if i <= 64 or i >= 91 and i <= 96 or i >= 123)\n",
    "    stopwords = nltk.corpus.stopwords.words('english')\n",
    "    stopwords.extend(char_blacklist)\n",
    "    df = pd.read_csv('cleaned_data.csv')\n",
    "    \n",
    "    top = 50\n",
    "    words_frequency = {}\n",
    "    for category in set(df['main_category'].values):\n",
    "        all_words = []\n",
    "        for row in df[df['main_category'] == category]['tokenized_words'].tolist():\n",
    "            for word in ast.literal_eval(row):\n",
    "                all_words.append(word)\n",
    "        allWordExceptStopDist = nltk.FreqDist(\n",
    "            w.lower() for w in all_words if w not in stopwords and len(w) >= 3 and w[0] not in char_blacklist)\n",
    "\n",
    "        most_common = allWordExceptStopDist.most_common(top)\n",
    "        words_frequency[category] = most_common\n",
    "    \n",
    "    for category in set(df['main_category'].values):\n",
    "        words_frequency[category] = [word for word, number in words_frequency[category]]\n",
    "        \n",
    "    from collections import Counter\n",
    "    \n",
    "    features = np.zeros(df.shape[0] * top).reshape(df.shape[0], top)\n",
    "    labels = np.zeros(df.shape[0])\n",
    "    counter = 0\n",
    "    for i, row in df.iterrows():\n",
    "        c = [word for word, word_count in Counter(ast.literal_eval(row['tokenized_words'])).most_common(top)]\n",
    "        labels[counter] = list(set(df['main_category'].values)).index(row['main_category'])\n",
    "        for word in c:\n",
    "            if word in words_frequency[row['main_category']]:\n",
    "                features[counter][words_frequency[row['main_category']].index(word)] = 1\n",
    "        counter += 1\n",
    "        \n",
    "    return labels, features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def no_filter_data():\n",
    "    file = 'URL-categorization-DFE.csv'\n",
    "    df = pd.read_csv(file)[['main_category', 'main_category:confidence', 'url']]\n",
    "    df = df[(df['main_category'] != 'Not_working') & (df['main_category:confidence'] > 0.5)]\n",
    "    df['tokenized_words'] = ''\n",
    "    \n",
    "    counter = 0\n",
    "    for i, row in df.iterrows():\n",
    "        counter += 1\n",
    "        print(\"{}, {}/{}\".format(row['url'], counter, len(df)))\n",
    "\n",
    "        try:\n",
    "            html = urlopen('http://' + row['url'], timeout=1).read()\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "        soup = BeautifulSoup(html, \"html.parser\")\n",
    "        for script in soup([\"script\", \"style\"]):\n",
    "            script.extract()\n",
    "        text = soup.get_text()\n",
    "        lines = (line.strip() for line in text.splitlines())\n",
    "        chunks = (phrase.strip() for line in lines for phrase in line.split(\"  \"))\n",
    "        text = '\\n'.join(chunk.lower() for chunk in chunks if chunk)\n",
    "        tokens = nltk.word_tokenize(text)\n",
    "        \n",
    "        df.at[i, 'tokenized_words'] = tokens if len(tokens) > 0 else ''\n",
    "        \n",
    "    df = df[df['tokenized_words'] != '']\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if os.path.isfile(\"cleaned_data.csv\"):\n",
    "#     labels, features = train_machine()\n",
    "# else:\n",
    "#     conctruct_dataset()\n",
    "#     labels, features = train_machine()\n",
    "if not os.path.isfile(\"Datasets/full_data.csv\"):\n",
    "    df = no_filter_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-633337079cd0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from scipy.sparse import coo_matrix\n",
    "X_sparse = coo_matrix(features)\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "X, X_sparse, y = shuffle(features, X_sparse, labels, random_state=0)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12. 12. 15. ...  1.  1. 18.]\n",
      "0.42168674698795183\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "lr = LogisticRegression()\n",
    "lr.fit(X_train, y_train)\n",
    "predictions = lr.predict(X_test)\n",
    "score = lr.score(X_test, y_test)\n",
    "print(predictions)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 9. 24.  4. ...  1. 19. 18.]\n",
      "0.2754350736278447\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "dtc = DecisionTreeClassifier()\n",
    "dtc.fit(X_train, y_train)\n",
    "predictions = dtc.predict(X_test)\n",
    "score = dtc.score(X_test, y_test)\n",
    "print(predictions)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12. 12. 15. ...  1.  1. 18.]\n",
      "0.3989290495314592\n"
     ]
    }
   ],
   "source": [
    "from sklearn import svm\n",
    "svm = svm.SVC()\n",
    "svm.fit(X_train, y_train)\n",
    "predictions = svm.predict(X_test)\n",
    "score = svm.score(X_test, y_test)\n",
    "print(predictions)\n",
    "print(score)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
