{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "import csv\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup\n",
    "from langdetect import detect\n",
    "\n",
    "\n",
    "# nltk.download('punk')\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "file = '/home/domantas/Desktop/Erasmus/MachineLearning/url_classification_1project/url_test'\n",
    "url_counter = 1\n",
    "reader = csv.reader(open(file), delimiter=',')\n",
    "header = next(reader)\n",
    "char_blacklist = list(chr(i) for i in range(32, 127) if i <= 64 or i >= 91 and i <= 96 or i >= 123)\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "stopwords.extend(char_blacklist)\n",
    "language_blacklist = ['ru', 'vi', 'zh-cn', 'ja', 'ko', 'fa', 'ar', 'et', 'tr', 'bn', 'el', 'sk', 'sv', 'pl']\n",
    "english_vocab = set(w.lower() for w in nltk.corpus.words.words())\n",
    "top = 5\n",
    "counter = 1\n",
    "data = []\n",
    "tokens_list = []\n",
    "freq_words = []\n",
    "filter_data = []\n",
    "filter_rows = []\n",
    "labels = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for row in reader:\n",
    "    data.append(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----\n",
      "URL: 000webhost.com\n",
      "CATEGORY: Internet_and_Telecom\n",
      "1\n",
      "Nr:  2\n",
      "----\n",
      "URL: 0calc.com\n",
      "CATEGORY: Science\n",
      "2\n",
      "Nr:  12\n",
      "----\n",
      "URL: 1000ps.at\n",
      "CATEGORY: Autos_and_Vehicles\n",
      "3\n",
      "Nr:  19\n",
      "----\n",
      "URL: 10bet.com\n",
      "CATEGORY: Gambling\n",
      "4\n",
      "Nr:  28\n",
      "----\n",
      "URL: 1100ad.com\n",
      "CATEGORY: Books_and_Literature\n",
      "5\n",
      "Nr:  30\n"
     ]
    }
   ],
   "source": [
    "for row in data:\n",
    "    counter += 1\n",
    "    if row[5] != 'Not_working' and float(row[6]) > 0.5:\n",
    "        try:\n",
    "            url = 'http://' + row[-1]\n",
    "            html = urlopen(url, timeout=1).read()\n",
    "            soup = BeautifulSoup(html, \"html.parser\")\n",
    "            for script in soup([\"script\", \"style\"]):\n",
    "                script.extract()\n",
    "            text = soup.get_text()\n",
    "            lines = (line.strip() for line in text.splitlines())\n",
    "            chunks = (phrase.strip() for line in lines for phrase in line.split(\"  \"))\n",
    "            text = '\\n'.join(chunk.lower() for chunk in chunks if chunk)\n",
    "            text_vocab = set(w.lower() for w in text if w.lower().isalpha())\n",
    "            if detect(text) != 'en':\n",
    "                continue\n",
    "            tokens = nltk.word_tokenize(text)\n",
    "            tokens_list += [nltk.word_tokenize(text)]\n",
    "            print('----')\n",
    "            print('URL: ' + row[-1])\n",
    "            print('CATEGORY: ' + row[5])\n",
    "            print(url_counter)\n",
    "            print('Nr: ', counter)\n",
    "            filter_data += [row[5]]\n",
    "            url_counter += 1\n",
    "            filter_rows += [row]\n",
    "\n",
    "        except:\n",
    "            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_categories = list(set(filter_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in filter_rows:\n",
    "    labels += [all_categories.index(i[5])]\n",
    "labels = np.array(labels).reshape((len(tokens_list), 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for tokens in tokens_list:\n",
    "    allWordDist = nltk.FreqDist(w.lower() for w in tokens)\n",
    "    allWordExceptStopDist = nltk.FreqDist(\n",
    "        w.lower() for w in tokens if w not in stopwords and len(w) >= 3 and w[0] not in char_blacklist)\n",
    "    all_words = [i for i in allWordExceptStopDist]\n",
    "    mostCommon = allWordExceptStopDist.most_common(top)\n",
    "    freq_words += [word for word, number in mostCommon]\n",
    "    print(mostCommon)\n",
    "print(freq_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "features = np.zeros(pow(len(tokens_list), 2) * top).reshape(len(tokens_list), len(tokens_list) * top)\n",
    "counter = 0\n",
    "for line in tokens_list:\n",
    "    for word in line:\n",
    "        if word in freq_words:\n",
    "            # features[counter][freq_words.index(word)] += 1\n",
    "            features[counter][freq_words.index(word)] = 1\n",
    "    counter += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(features)\n",
    "# print(labels)\n",
    "lr = LogisticRegression()\n",
    "prediction = cross_val_predict(lr, features, labels, cv=5)\n",
    "print(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
