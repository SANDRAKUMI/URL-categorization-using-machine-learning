{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/domantas/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package words to /home/domantas/nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/domantas/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "import ast\n",
    "import numpy as np\n",
    "import os\n",
    "import ast\n",
    "import urllib.request\n",
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup\n",
    "import os.path\n",
    "nltk.download('stopwords')\n",
    "nltk.download('words')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset creation if it is not existing.\n",
    "__Dataset is filtered by these set of rules:__\n",
    "1. Main category != Not_working (Exclude non working URL's)\n",
    "2. Main category:confidence > 0.5 (Leave url's with likely know categories)\n",
    "3. Non responding URL's are excluded\n",
    "4. Non english language URL's are excluded.\n",
    "\n",
    "### Caution, the full data set creation may take ~15 hours."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def no_filter_data():\n",
    "    file = 'Datasets/URL-categorization-DFE.csv'\n",
    "    df = pd.read_csv(file)[['main_category', 'main_category:confidence', 'url']]\n",
    "    df = df[(df['main_category'] != 'Not_working') & (df['main_category:confidence'] > 0.5)]\n",
    "    df['tokenized_words'] = ''\n",
    "    \n",
    "    counter = 0\n",
    "    for i, row in df.iterrows():\n",
    "        counter += 1\n",
    "        print(\"{}, {}/{}\".format(row['url'], counter, len(df)))\n",
    "\n",
    "        try:\n",
    "            hdr = {'User-Agent': 'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.11 (KHTML, like Gecko) Chrome/23.0.1271.64 Safari/537.11',\n",
    "               'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',\n",
    "               'Accept-Charset': 'ISO-8859-1,utf-8;q=0.7,*;q=0.3',\n",
    "               'Accept-Encoding': 'none',\n",
    "               'Accept-Language': 'en-US,en;q=0.8',\n",
    "               'Connection': 'keep-alive'}\n",
    "            req = urllib.request.Request(url, headers=hdr)\n",
    "            html = urlopen(req).read()\n",
    "#             html = urlopen('http://' + row['url'], timeout=15).read()\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "        soup = BeautifulSoup(html, \"html.parser\")\n",
    "        [tag.decompose() for tag in soup(\"script\")]\n",
    "        [tag.decompose() for tag in soup(\"style\")]\n",
    "        text = soup.get_text()\n",
    "        lines = (line.strip() for line in text.splitlines())\n",
    "        chunks = (phrase.strip() for line in lines for phrase in line.split(\"  \"))\n",
    "        text = '\\n'.join(chunk.lower() for chunk in chunks if chunk)\n",
    "        tokens = nltk.word_tokenize(text)\n",
    "\n",
    "        df.at[i, 'tokenized_words'] = tokens if len(tokens) > 0 else ''\n",
    "        \n",
    "    df = df[~df['tokenized_words'].isnull()]\n",
    "    df.to_csv(\"Datasets/full_data_v3.csv\")\n",
    "    \n",
    "if not os.path.isfile(\"Datasets/full_data_v3.csv\"):\n",
    "    no_filter_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading data set and creating list of stopwords and english vocabulary for further investigation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"Datasets/full_data_december.csv\")\n",
    "df = df[~df['tokenized_words'].isnull()]\n",
    "char_blacklist = list(chr(i) for i in range(32, 127) if i <= 64 or i >= 91 and i <= 96 or i >= 123)\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "stopwords.extend(char_blacklist)\n",
    "english_vocab = set(w.lower() for w in nltk.corpus.words.words())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filter webpages with english language\n",
    "If the webpage contains at least 20 % english words of total words, then the webpage is considered as english"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "english_confidence = []\n",
    "english_tolerance = 50\n",
    "for i, row in df.iterrows():\n",
    "    english_words = 0\n",
    "    words = ast.literal_eval(row['tokenized_words'])\n",
    "    for word in words:\n",
    "        if word.lower() in english_vocab:\n",
    "            english_words += 1\n",
    "    english_confidence.append(english_words / len(words) * 100)\n",
    "df['english:confidence'] = english_confidence\n",
    "df = df[df['english:confidence'] > english_tolerance]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make the most popular word list for each catgegory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "top = 2500\n",
    "words_frequency = {}\n",
    "for category in set(df['main_category'].values):\n",
    "    all_words = []\n",
    "    for row in df[df['main_category'] == category]['tokenized_words'].tolist():\n",
    "        for word in ast.literal_eval(row):\n",
    "            all_words.append(word)\n",
    "                \n",
    "    allWordExceptStopDist = nltk.FreqDist(\n",
    "        w.lower() for w in all_words if w not in stopwords and len(w) >= 3 and w[0] not in char_blacklist)\n",
    "\n",
    "    most_common = allWordExceptStopDist.most_common(top)\n",
    "    words_frequency[category] = most_common\n",
    "\n",
    "for category in set(df['main_category'].values):\n",
    "    words_frequency[category] = [word for word, number in words_frequency[category]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove most frequent words in all categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "words = []\n",
    "for category in words_frequency.keys():\n",
    "    words.extend(words_frequency[category][0:15])\n",
    "words_counter = Counter(words)\n",
    "words_filter = {x : words_counter[x] for x in words_counter if words_counter[x] >= 7}\n",
    "words_stop = list(words_filter.keys())\n",
    "for category in words_frequency.keys():\n",
    "    words_frequency[category] = [word for word in words_frequency[category] if word not in words_stop]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'new': 22,\n",
       " 'home': 7,\n",
       " 'view': 7,\n",
       " 'read': 13,\n",
       " 'learn': 7,\n",
       " 'information': 8,\n",
       " 'contact': 14,\n",
       " 'free': 7,\n",
       " 'news': 9,\n",
       " 'get': 7}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words_filter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create features and labels for Machine learning training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "features = np.zeros(df.shape[0] * top).reshape(df.shape[0], top)\n",
    "labels = np.zeros(df.shape[0])\n",
    "counter = 0\n",
    "for i, row in df.iterrows():\n",
    "    c = [word for word, word_count in Counter(ast.literal_eval(row['tokenized_words'])).most_common(top)]\n",
    "    labels[counter] = list(set(df['main_category'].values)).index(row['main_category'])\n",
    "    for word in c:\n",
    "        if word in words_frequency[row['main_category']]:\n",
    "            features[counter][words_frequency[row['main_category']].index(word)] = 1\n",
    "    counter += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create seperate training/testing datasets and shuffle them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from scipy.sparse import coo_matrix\n",
    "X_sparse = coo_matrix(features)\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "X, X_sparse, y = shuffle(features, X_sparse, labels, random_state=0)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/domantas/.local/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/home/domantas/.local/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:460: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression\n",
      "Score:  0.8388802488335926\n",
      "Top:  2500\n",
      "Tolerance:  50\n",
      "Dataset length:  9740\n",
      "\n",
      "DecisionTreeClassifier\n",
      "Score:  0.3244167962674961\n",
      "Top:  2500\n",
      "Tolerance:  50\n",
      "Dataset length:  9740\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/domantas/.local/lib/python3.6/site-packages/sklearn/svm/base.py:922: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM\n",
      "Score:  0.728149300155521\n",
      "Top:  2500\n",
      "Tolerance:  50\n",
      "Dataset length:  9740\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "lr = LogisticRegression()\n",
    "lr.fit(X_train, y_train)\n",
    "predictions = lr.predict(X_test)\n",
    "score = lr.score(X_test, y_test)\n",
    "print('LogisticRegression')\n",
    "print('Score: ', score)\n",
    "print('Top: ', top)\n",
    "print('Tolerance: ', english_tolerance)\n",
    "print('Dataset length: ', df.shape[0])\n",
    "print()\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "dtc = DecisionTreeClassifier()\n",
    "dtc.fit(X_train, y_train)\n",
    "predictions = dtc.predict(X_test)\n",
    "score = dtc.score(X_test, y_test)\n",
    "print('DecisionTreeClassifier')\n",
    "print('Score: ', score)\n",
    "print('Top: ', top)\n",
    "print('Tolerance: ', english_tolerance)\n",
    "print('Dataset length: ', df.shape[0])\n",
    "print()\n",
    "from sklearn.svm import LinearSVC\n",
    "clf = LinearSVC()\n",
    "clf.fit(X_train, y_train) \n",
    "predictions = clf.predict(X_test)\n",
    "score = clf.score(X_test, y_test)\n",
    "print('SVM')\n",
    "print('Score: ', score)\n",
    "print('Top: ', top)\n",
    "print('Tolerance: ', english_tolerance)\n",
    "print('Dataset length: ', df.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save ML model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/domantas/.local/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/home/domantas/.local/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:460: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "month = 'December'\n",
    "from sklearn.externals import joblib\n",
    "filename = \"Models/{}/LR_model_v3_stop_{}.joblib\".format(month, month)\n",
    "if not os.path.isfile(filename):\n",
    "    joblib.dump(lr, filename) \n",
    "\n",
    "import pickle\n",
    "words_filename = \"Models/{}/word_frequency_v3_stop_{}.picle\".format(month, month)\n",
    "if not os.path.isfile(words_filename):\n",
    "    pickle_out = open(words_filename,\"wb\")\n",
    "    pickle.dump(words_frequency, pickle_out)\n",
    "    pickle_out.close()\n",
    "    \n",
    "filename = \"Models/{}/LR_maxtrain_v3.joblib_stop_{}\".format(month, month)\n",
    "if not os.path.isfile(filename):\n",
    "    from sklearn.linear_model import LogisticRegression\n",
    "    lr = LogisticRegression()\n",
    "    lr.fit(X, y)\n",
    "    joblib.dump(lr, filename) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt; plt.rcdefaults()\n",
    "# import numpy as np\n",
    "# import matplotlib.pyplot as plt\n",
    " \n",
    "# objects = ('English', 'Italic', 'Russian', 'Japan', 'China', 'Belgium')\n",
    "# y_pos = np.arange(len(objects))\n",
    "# performance = [8143,260,646,338,125,100]\n",
    " \n",
    "# plt.bar(y_pos, performance, align='center', alpha=0.5)\n",
    "# plt.xticks(y_pos, objects)\n",
    "# plt.ylabel('URLs')\n",
    "# plt.title('Languages diversity in the data set')\n",
    " \n",
    "# plt.show()\n",
    "# plt.savefig(\"language_diversity.png\")\n",
    "# df[df['main_category'] == 'Business_and_Industry']['url']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt; plt.rcdefaults()\n",
    "# import numpy as np\n",
    "# import matplotlib.pyplot as plt\n",
    "# from collections import Counter\n",
    "\n",
    "# words = []\n",
    "# for category in words_frequency.keys():\n",
    "#     words.extend(words_frequency[category][0:15])\n",
    "# words_counter = Counter(words)\n",
    "# words_filter = {x : words_counter[x] for x in words_counter if words_counter[x] >= 7}\n",
    "# objects = tuple(words_filter.keys())\n",
    "# y_pos = np.arange(len(objects))\n",
    "# performance = list(words_filter.values())\n",
    "\n",
    "# plt.barh(y_pos, performance, align='center', alpha=1)\n",
    "# plt.xticks(range(1, max(performance) + 1))\n",
    "# plt.yticks(y_pos, objects)\n",
    "# plt.xlabel('Word diversity in categories (TOP 15 words)')\n",
    "# plt.title('Words diversity in each category TOP 15 most frequent words')\n",
    " \n",
    "# plt.show()\n",
    "# plt.savefig(\"words_diversity.png\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
