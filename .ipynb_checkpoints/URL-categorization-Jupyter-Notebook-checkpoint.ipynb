{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import libraries\n",
    "These libraries will be used for our URL_classification project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import csv\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import ast\n",
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "print(datetime.datetime.now().time())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Use this command if you have any errors on importing nltk library. It will open a nltk meniu with download and update options. If it's still missing some libraries, it needs to install manually by writing nltk.download('library name') where library name is missing library name which asserts error message."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('stopwords')\n",
    "nltk.download('words')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Setup\n",
    "Determine file path with all URL_classification data, set how many lines we want to read(limiter);\n",
    "top - a number which represents how many most frequent words is stored for each category.\n",
    "char_blacklist, stopwords, language_whitelist, domains_whitelist, english_vocab - these variables are for URL filtering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = 'URL-categorization-DFE.csv'\n",
    "df = pd.read_csv(file)[['main_category', 'main_category:confidence', 'url']]\n",
    "df = df[(df['main_category'] != 'Not_working') & (df['main_category:confidence'] > 0.5)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "char_blacklist = list(chr(i) for i in range(32, 127) if i <= 64 or i >= 91 and i <= 96 or i >= 123)\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "stopwords.extend(char_blacklist)\n",
    "language_whitelist = ['en']\n",
    "english_vocab = set(w.lower() for w in nltk.corpus.words.words())\n",
    "blacklist_domain = ['.it', '.ru', '.cn', '.jp', '.tw', '.de', '.pl', '.fr', '.hu', '.bg', '.nl']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter out all non english language domains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[~df['url'].str.endswith(tuple(blacklist_domain))]\n",
    "df['tokenized_words'] = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counter = 0\n",
    "for i, row in df.iterrows():\n",
    "    counter += 1\n",
    "    if counter >= 50:\n",
    "        break\n",
    "    print(\"{}, {}/{}\".format(row['url'], counter, len(df)))\n",
    "    \n",
    "    try:\n",
    "        html = urlopen('http://' + row['url'], timeout=1).read()\n",
    "    except:\n",
    "        continue\n",
    "        \n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "    for script in soup([\"script\", \"style\"]):\n",
    "        script.extract()\n",
    "    text = soup.get_text()\n",
    "    lines = (line.strip() for line in text.splitlines())\n",
    "    chunks = (phrase.strip() for line in lines for phrase in line.split(\"  \"))\n",
    "    text = '\\n'.join(chunk.lower() for chunk in chunks if chunk)\n",
    "    filter_text = \" \".join(w for w in nltk.word_tokenize(text) \\\n",
    "         if w.lower() in english_vocab)\n",
    "    \n",
    "    tokens = nltk.word_tokenize(filter_text)\n",
    "    \n",
    "    allWordExceptStopDist = nltk.FreqDist(\n",
    "        w.lower() for w in tokens if w not in stopwords and len(w) >= 3 and w[0] not in char_blacklist)\n",
    "    \n",
    "    all_words = [i for i in allWordExceptStopDist]\n",
    "\n",
    "    if len(all_words) > 0:\n",
    "        continue\n",
    "        \n",
    "    df.at[i, 'tokenized_words'] = all_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df['tokenized_words'] != '']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(datetime.datetime.now().time())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('cleaned_data.csv')\n",
    "# df = df[df['main_category'].isin(['Health', 'Food_and_Drink'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [],
   "source": [
    "top = 50\n",
    "words_frequency = {}\n",
    "for category in set(df['main_category'].values):\n",
    "    all_words = []\n",
    "    for row in df[df['main_category'] == category]['tokenized_words'].tolist():\n",
    "        for word in ast.literal_eval(row):\n",
    "            all_words.append(word)\n",
    "    allWordExceptStopDist = nltk.FreqDist(\n",
    "        w.lower() for w in all_words if w not in stopwords and len(w) >= 3 and w[0] not in char_blacklist)\n",
    "\n",
    "    most_common = allWordExceptStopDist.most_common(top)\n",
    "    words_frequency[category] = most_common"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [],
   "source": [
    "for category in set(df['main_category'].values):\n",
    "    words_frequency[category] = [word for word, number in words_frequency[category]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set(words_frequency['Reference']) & set(words_frequency['Adult']) & set(words_frequency['Arts_and_Entertainment'])\\\n",
    "#     & set(words_frequency['Autos_and_Vehicles']) & set(words_frequency['Beauty_and_Fitness'])\\\n",
    "#     & set(words_frequency['Books_and_Literature']) & set(words_frequency['Business_and_Industry'])\\\n",
    "#     & set(words_frequency['Career_and_Education']) & set(words_frequency['Computer_and_Electronics'])\\\n",
    "#     & set(words_frequency['Finance']) & set(words_frequency['Food_and_Drink'])\\\n",
    "#     & set(words_frequency['Gambling']) & set(words_frequency['Games'])\\\n",
    "#     & set(words_frequency['Health']) & set(words_frequency['Home_and_Garden'])\\\n",
    "#     & set(words_frequency['Internet_and_Telecom']) & set(words_frequency['Law_and_Government'])\\\n",
    "#     & set(words_frequency['News_and_Media']) & set(words_frequency['People_and_Society'])\\\n",
    "#     & set(words_frequency['Pets_and_Animals']) & set(words_frequency['Recreation_and_Hobbies'])\\\n",
    "#     & set(words_frequency['Science']) & set(words_frequency['Shopping'])\\\n",
    "#     & set(words_frequency['Sports']) & set(words_frequency['Travel'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = np.zeros(df.shape[0] * top).reshape(df.shape[0], top)\n",
    "labels = np.zeros(df.shape[0])\n",
    "counter = 0\n",
    "for i, row in df.iterrows():\n",
    "    c = [word for word, word_count in Counter(ast.literal_eval(row['tokenized_words'])).most_common(top)]\n",
    "    labels[counter] = list(set(df['main_category'].values)).index(row['main_category'])\n",
    "    for word in c:\n",
    "        if word in words_frequency[row['main_category']]:\n",
    "            features[counter][words_frequency[row['main_category']].index(word)] = 1\n",
    "    counter += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse import coo_matrix\n",
    "X_sparse = coo_matrix(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle\n",
    "X, X_sparse, y = shuffle(features, X_sparse, labels, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  2.   2.  12. ...,   7.   7.  13.]\n",
      "0.421686746988\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "lr = LogisticRegression()\n",
    "lr.fit(X_train, y_train)\n",
    "predictions = lr.predict(X_test)\n",
    "score = lr.score(X_test, y_test)\n",
    "print(predictions)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 20.   3.   6. ...,   4.  19.  13.]\n",
      "0.270080321285\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "dtc = DecisionTreeClassifier()\n",
    "dtc.fit(X_train, y_train)\n",
    "predictions = dtc.predict(X_test)\n",
    "score = dtc.score(X_test, y_test)\n",
    "print(predictions)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  2.   2.  12. ...,   7.   7.  13.]\n",
      "0.397590361446\n"
     ]
    }
   ],
   "source": [
    "from sklearn import svm\n",
    "svm = svm.SVC()\n",
    "svm.fit(X_train, y_train)\n",
    "predictions = svm.predict(X_test)\n",
    "score = svm.score(X_test, y_test)\n",
    "print(predictions)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Category: Career_and_Education ; Entries: 263\n",
      "Category: Games ; Entries: 199\n",
      "Category: Autos_and_Vehicles ; Entries: 565\n",
      "Category: Books_and_Literature ; Entries: 573\n",
      "Category: Arts_and_Entertainment ; Entries: 151\n",
      "Category: Internet_and_Telecom ; Entries: 282\n",
      "Category: People_and_Society ; Entries: 426\n",
      "Category: Science ; Entries: 373\n",
      "Category: Law_and_Government ; Entries: 587\n",
      "Category: Recreation_and_Hobbies ; Entries: 143\n",
      "Category: Pets_and_Animals ; Entries: 129\n",
      "Category: Reference ; Entries: 578\n",
      "Category: Food_and_Drink ; Entries: 757\n",
      "Category: Finance ; Entries: 391\n",
      "Category: Health ; Entries: 732\n",
      "Category: Computer_and_Electronics ; Entries: 351\n",
      "Category: Adult ; Entries: 54\n",
      "Category: Travel ; Entries: 282\n",
      "Category: Shopping ; Entries: 207\n",
      "Category: Gambling ; Entries: 385\n",
      "Category: Business_and_Industry ; Entries: 295\n",
      "Category: Home_and_Garden ; Entries: 39\n",
      "Category: Sports ; Entries: 510\n",
      "Category: Beauty_and_Fitness ; Entries: 526\n",
      "Category: News_and_Media ; Entries: 254\n"
     ]
    }
   ],
   "source": [
    "for category in set(df['main_category'].values):\n",
    "    print(\"Category: {} ; Entries: {}\".format(category, len(df[df['main_category'] == category])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-274-2c61fd203262>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-274-2c61fd203262>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    Health Food_and_Drink\u001b[0m\n\u001b[0m                        ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "Health Food_and_Drink "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
